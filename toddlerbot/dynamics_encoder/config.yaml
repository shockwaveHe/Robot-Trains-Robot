device: 0
seed: 3407
run_mode: train

data:
  num_envs: 4
  num_test_envs: 4
  num_test_steps: 5
  batch_size: 32
  num_workers: 8
  horizon: 5
  history: 5
  transition_time_str: "20250305_135053"
  params_time_str: "20250305_135540"

dynamics:
  observation_size: 83
  action_size: 12
  hidden_layers: [128, 128, 128]

autoencoder:
  n_embd: 1024
  encoder_depth: 1
  decoder_depth: 1
  input_noise_factor: 0.001
  latent_noise_factor: 0.01
  is_vae: False

train:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-3
    weight_decay: 5e-3

  lr_scheduler:
  finetune: False

  loss_func:
    _target_: torch.nn.MSELoss
    reduction: mean

  trainer:
    _target_:  pytorch_lightning.trainer.Trainer
    strategy: 'auto'
    max_epochs: 50
    check_val_every_n_epoch: True
    val_check_interval: 
    log_every_n_steps: 10
    limit_val_batches: 1

    enable_model_summary: false

    callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      filename: "{epoch}-{val_loss:.4f}"
      monitor: 'val_loss'
      mode: 'min'
      save_top_k: 3
      verbose: true
      # - _target_: pytorch_lightning.callbacks.EarlyStopping
      # monitor: 'val_loss'
      # min_delta: 0.0001
      # patience: 10
      # verbose: True
      # mode: "min"

hydra:
  sweep:
    dir: "results/dynamics_combined_${now:%Y%m%d_%H%M%S}"
  run:
    dir: "results/dynamics_encoder_${now:%Y%m%d_%H%M%S}"