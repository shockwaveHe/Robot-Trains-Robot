algorithm:
  class_name: PPO
  # training parameters
  # -- advantage normalization
  normalize_advantage_per_mini_batch: False
  # -- value function
  value_loss_coef: 1.0
  clip_param: 0.1
  use_clipped_value_loss: true
  # -- surrogate loss
  desired_kl: 0.01
  entropy_coef: 0.01
  gamma: 0.99
  lam: 0.95
  max_grad_norm: 1.0
  # -- training
  learning_rate: 0.001
  num_learning_epochs: 5
  num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches
  schedule: adaptive  # adaptive, fixed

  # -- Random Network Distillation
  # rnd_cfg:
  #     weight: 0.0  # initial weight of the RND reward

  #     # note: This is a dictionary with a required key called "mode".
  #     #   Please check the RND module for more information.
  #     weight_schedule: null

  #     reward_normalization: false  # whether to normalize RND reward
  #     state_normalization: true  # whether to normalize RND state observations

  #     # -- Learning parameters
  #     learning_rate: 0.001  # learning rate for RND

  #     # -- Network parameters
  #     # note: if -1, then the network will use dimensions of the observation
  #     num_outputs: 1  # number of outputs of RND network
  #     predictor_hidden_dims: [-1] # hidden dimensions of predictor network
  #     target_hidden_dims: [-1]  # hidden dimensions of target network


policy:
  class_name: ActorCritic
  # for MLP i.e. `ActorCritic`
  activation: elu
  actor_hidden_dims: [128, 128, 128]
  critic_hidden_dims: [128, 128, 128]
  init_noise_std: 1.0
  noise_std_type: "log"  # 'scalar' or 'log'

  # only needed for `ActorCriticRecurrent`
  # rnn_type: 'lstm'
  # rnn_hidden_size: 512
  # rnn_num_layers: 1

runner:
    num_steps_per_env: 24  # number of steps per environment per iteration
    max_iterations: 1500  # number of policy updates
    normalization_type: "none" # running_mean_std, empirical, none
    # -- logging parameters
    save_interval: 100  # check for potential saves every `save_interval` iterations
    experiment_name: ""
    run_name: ""
    # -- logging writer
    logger: wandb  # tensorboard, neptune, wandb
    wandb_project: RTR
    # -- load and resuming
    resume: false
    load_run: -1  # -1 means load latest run
    resume_path: null  # updated from load_run and checkpoint
    checkpoint: -1  # -1 means load latest checkpoint
    train_render_freq: 1000
    eval_freq: 100
    eval_render_freq: 1000
    eval_steps_per_env: 1000

runner_class_name: OnPolicyRunner
seed: 1
